{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone Git"
      ],
      "metadata": {
        "id": "fNIr26bs_mgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fabianagoes/bc2_tutorial8.git"
      ],
      "metadata": {
        "id": "xeHNGKgF_lWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "q9_rdFo8S-hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing Functions"
      ],
      "metadata": {
        "id": "-0l9BZ-YToUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "SDsgTdNFXOdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "dyH5JbfnXK3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp7rGdwAS6bq"
      },
      "outputs": [],
      "source": [
        "def k_mers(sequences, k=3):\n",
        "    results = []\n",
        "    for seq in sequences:\n",
        "        kmers = [seq[i:i+k] for i in range(0, len(seq), k)]\n",
        "        results.append(kmers)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"k_mers\": results})\n",
        "\n",
        "\n",
        "def sliding_k_mers(sequences, k=3):\n",
        "    results = []\n",
        "    for seq in sequences:\n",
        "        kmers = [seq[i:i+k] for i in range(len(seq)-k+1)] if len(seq) >= k else []\n",
        "        results.append(kmers)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"sliding_k_mers\": results})\n",
        "\n",
        "\n",
        "def simple_bpe(sequences, num_merges=2):\n",
        "    results = []\n",
        "    for seq in sequences:\n",
        "        tokens = list(seq)\n",
        "        for _ in range(num_merges):\n",
        "            pairs = [tokens[i]+tokens[i+1] for i in range(len(tokens)-1)]\n",
        "            if not pairs:\n",
        "                break\n",
        "            most_common = Counter(pairs).most_common(1)[0][0]\n",
        "            i = 0\n",
        "            new_tokens = []\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens)-1 and tokens[i]+tokens[i+1] == most_common:\n",
        "                    new_tokens.append(tokens[i]+tokens[i+1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "        results.append(tokens)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"bpe_tokens\": results})\n",
        "\n",
        "\n",
        "def naive(sequences):\n",
        "    results = [list(seq) for seq in sequences]\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"naive_tokens\": results})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Functions"
      ],
      "metadata": {
        "id": "fQJJWdjlTtrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(sequences):\n",
        "    results = []\n",
        "    for seq in sequences:\n",
        "        tokens = list(seq)\n",
        "        vocab = sorted(set(tokens))\n",
        "        token_to_idx = {token: i for i, token in enumerate(vocab)}\n",
        "        one_hot_matrix = np.eye(len(vocab))[[token_to_idx[t] for t in tokens]]\n",
        "        results.append(one_hot_matrix)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"one_hot\": results})\n",
        "\n",
        "\n",
        "def positional_encoding(sequences, d_model):\n",
        "    results = []\n",
        "    for seq in sequences:\n",
        "        seq_len = len(seq)\n",
        "        pos = np.arange(seq_len)[:, np.newaxis]\n",
        "        i = np.arange(d_model)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / d_model)\n",
        "        pe = pos * angle_rates\n",
        "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = np.cos(pe[:, 1::2])\n",
        "        results.append(pe)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"positional_encoding\": results})\n",
        "\n",
        "\n",
        "def word2vec_embeddings(sequences, k):\n",
        "    results = []\n",
        "    for seq in sequences:\n",
        "        kmers = [seq[i:i+k] for i in range(0, len(seq)-k+1)]\n",
        "        sentences = [kmers]\n",
        "        w2v_model = Word2Vec(sentences, vector_size=8, window=2, min_count=1, sg=1)\n",
        "        w2v_emb = np.array([w2v_model.wv[k] for k in kmers])\n",
        "        results.append(w2v_emb)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"word2vec\": results})\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dna_tokenizer = BertTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\")\n",
        "dna_model = BertModel.from_pretrained(\"zhihan1996/DNA_bert_6\").to(device)\n",
        "\n",
        "nt_model_name = \"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\"\n",
        "nt_tokenizer = AutoTokenizer.from_pretrained(nt_model_name)\n",
        "nt_model = AutoModel.from_pretrained(nt_model_name).to(device)\n"
      ],
      "metadata": {
        "id": "qDAS5eXcTxOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dna_bert(sequences, batch_size=16):\n",
        "    results = []\n",
        "    sequences=list(sequences)\n",
        "    for i in range(0, len(sequences), batch_size):\n",
        "        batch = sequences[i:i + batch_size]\n",
        "        #print(batch)\n",
        "        inputs = dna_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = dna_model(**inputs)\n",
        "        # outputs.last_hidden_state: (batch_size, seq_len, hidden_size)\n",
        "        batch_emb = [emb.squeeze(0).cpu().numpy() for emb in outputs.last_hidden_state]\n",
        "        results.extend(batch_emb)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"dna_bert\": results})\n",
        "\n",
        "\n",
        "def nt(sequences, batch_size=8):\n",
        "    results = []\n",
        "    sequences=list(sequences)\n",
        "    for i in range(0, len(sequences), batch_size):\n",
        "        batch = sequences[i:i + batch_size]\n",
        "        inputs = nt_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = nt_model(**inputs)\n",
        "        batch_emb = [emb.squeeze(0).cpu().numpy() for emb in outputs.last_hidden_state]\n",
        "        results.extend(batch_emb)\n",
        "    return pd.DataFrame({\"sequence\": sequences, \"nt_embedding\": results})"
      ],
      "metadata": {
        "id": "Nxyhk3jghlrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Tokens"
      ],
      "metadata": {
        "id": "kOIUco_EZgSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the DNA dataset and look at the data"
      ],
      "metadata": {
        "id": "K_q8oLvZwb1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv('/content/bc2_tutorial8/synthetic_dna_dataset.csv')\n",
        "sequences=dataset['Sequence']\n",
        "print(sequences.head(5))\n",
        "print()\n",
        "print('Length: ', len(sequences[0]))"
      ],
      "metadata": {
        "id": "wopitQxwXlU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the 5 types of tokens and print the tokens for the first sequence"
      ],
      "metadata": {
        "id": "NREgyBQXwiLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmer_tokens3=k_mers(sequences,k=3)\n",
        "kmer_tokens4=k_mers(sequences,k=4)\n",
        "sliding_kmer_tokens=sliding_k_mers(sequences)\n",
        "bpe_tokens=simple_bpe(sequences)\n",
        "naive_tokens=naive(sequences)\n",
        "print(sequences[0])\n",
        "print('==============================================================================================================')\n",
        "print('3-mer Tokens:         ',kmer_tokens3['k_mers'][0])\n",
        "print('4-mer Tokens:         ',kmer_tokens4['k_mers'][0])\n",
        "print('Sliding 3-mer Tokens: ',sliding_kmer_tokens['sliding_k_mers'][0])\n",
        "print('BPE Tokens:           ',bpe_tokens['bpe_tokens'][0])\n",
        "print('Naive Tokens:         ',naive_tokens['naive_tokens'][0])"
      ],
      "metadata": {
        "id": "0t8x0jI5aBzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Embeddings"
      ],
      "metadata": {
        "id": "v5Xn87aaqL3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Embeddings for 50 sequences"
      ],
      "metadata": {
        "id": "hLTMe0zbw4_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_10=sequences[:50]\n",
        "one_hot_emb=one_hot(sequences_10)\n",
        "pos_emb=positional_encoding(sequences_10,d_model=8)\n",
        "w2v_emb=word2vec_embeddings(sequences_10,k=3)\n",
        "dna_bert_emb=dna_bert(sequences_10)\n",
        "nt_emb=nt(sequences_10)\n",
        "\n"
      ],
      "metadata": {
        "id": "62fiPFWcbSRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Embeddings"
      ],
      "metadata": {
        "id": "taxOBXZ-w7lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Flatten embeddings helper\n",
        "def flatten_embeddings(df, col_name):\n",
        "    return np.array([emb.flatten() for emb in df[col_name]])\n",
        "\n",
        "# Prepare flattened embeddings\n",
        "embeddings_dict = {\n",
        "    \"One-Hot\": flatten_embeddings(one_hot_emb, \"one_hot\"),\n",
        "    \"Positional Encoding\": flatten_embeddings(pos_emb, \"positional_encoding\"),\n",
        "    \"Word2Vec k=3\": flatten_embeddings(w2v_emb, \"word2vec\"),\n",
        "    \"DNABERT\": flatten_embeddings(dna_bert_emb, \"dna_bert\"),\n",
        "    \"Nucleotide Transformer\": flatten_embeddings(nt_emb, \"nt_embedding\")\n",
        "}\n",
        "\n",
        "# Generate sequence labels as Seq1, Seq2, ...\n",
        "seqs = [i for i in range(len(one_hot_emb))]\n",
        "\n",
        "# Plot heatmaps\n",
        "fig, axes = plt.subplots(len(embeddings_dict), 1, figsize=(15, 3*len(embeddings_dict)))\n",
        "\n",
        "for ax, (title, emb) in zip(axes, embeddings_dict.items()):\n",
        "    im = ax.imshow(emb, aspect='auto', cmap='viridis')\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel(\"Sequences\")\n",
        "    ax.set_yticklabels(seqs)\n",
        "    ax.set_xlabel(\"Flattened Embedding Dimension\")\n",
        "    fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.02, pad=0.02)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FRndYlr4kCwY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}