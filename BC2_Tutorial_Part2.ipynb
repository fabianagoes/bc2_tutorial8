{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eee5b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Tutorial (Colab) â€” Fine-tuning step by step (English, no custom wrappers)\n",
    "# - No pair-task support (only single sequence classification: text,label)\n",
    "# - Three modes: full fine-tuning, head-only, or LoRA\n",
    "# - Expects train.csv, dev.csv and test.csv with format: text,label\n",
    "# =============================================================\n",
    "\n",
    "# If running on Google Colab, uncomment:\n",
    "# !pip install -q transformers peft datasets accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b361322",
   "metadata": {},
   "source": [
    "# 1) Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ea59f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "!git clone https://github.com/fabianagoes/bc2_tutorial8.git\n",
    "%cd bc2_tutorial8\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, csv, json, logging, random, warnings\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, precision_score, recall_score\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torch.cuda.amp.autocast.*\")\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d525e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038be2e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# zhihan1996/DNABERT-2-117M\n",
    "# InstaDeepAI/nucleotide-transformer-500m-human-ref\n",
    "model_name_or_path = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"  # example: \"bert-base-uncased\", \"InstaDeepAI/nucleotide-transformer-v2-50m-1000g\"\n",
    "\n",
    "# Training strategy (choose ONE)\n",
    "use_lora = False          # True for LoRA\n",
    "train_head_only = False   # True to train only the classification head\n",
    "# If both are False => full fine-tuning\n",
    "\n",
    "# LoRA parameters (if use_lora=True)\n",
    "lora_r = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = \"query,value\"  # Adjust for your model, e.g. \"q_proj,v_proj\"\n",
    "\n",
    "# Data\n",
    "data_path = \"/ceph/groups/aibds/fabiana/workspace1/bc2_tutorial/datasets/GUE/prom/prom_300_notata\"  # must contain train.csv, dev.csv, test.csv\n",
    "\n",
    "# Training args\n",
    "run_name = \"run\"\n",
    "output_dir = \"/ceph/groups/aibds/fabiana/workspace1/bc2_tutorial/output\"\n",
    "model_max_length = 512\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 16\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "save_strategy = \"epoch\"\n",
    "evaluation_strategy = \"epoch\"\n",
    "eval_steps = 100\n",
    "warmup_steps = 50\n",
    "weight_decay = 0.1\n",
    "learning_rate = 1e-4\n",
    "save_total_limit = None\n",
    "load_best_model_at_end = True\n",
    "seed = 42\n",
    "\n",
    "# Save model and results\n",
    "save_model = False\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c13ec",
   "metadata": {},
   "source": [
    "# 3) Seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae476a95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbaf62e",
   "metadata": {},
   "source": [
    "# 4) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0091d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if \"InstaDeepAI\" in model_name_or_path and tokenizer.eos_token is None and tokenizer.pad_token is not None:\n",
    "    tokenizer.eos_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1d9f7",
   "metadata": {},
   "source": [
    "# 5) Read CSVs (single sequence classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579fd16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nReading data...\")\n",
    "print(\"Task: single sequence classification (text,label)\")\n",
    "\n",
    "def stratified_sample(df, frac, label_col=\"label\", random_state=42):\n",
    "    return (\n",
    "        df.groupby(label_col, group_keys=False)\n",
    "          .apply(lambda x: x.sample(frac=frac, random_state=random_state))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "dev_df   = pd.read_csv(os.path.join(data_path, \"dev.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "train_df = stratified_sample(train_df, 0.01)\n",
    "dev_df   = stratified_sample(dev_df, 0.08)\n",
    "test_df  = stratified_sample(test_df, 0.08)\n",
    "\n",
    "print(\"\\nAfter stratified sampling:\")\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Dev size:\", len(dev_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "print(\"\\nClass distribution in train:\")\n",
    "print(train_df[\"label\"].value_counts())\n",
    "\n",
    "train_texts, train_labels = train_df[\"sequence\"].tolist(), train_df[\"label\"].astype(int).tolist()\n",
    "dev_texts, dev_labels     = dev_df[\"sequence\"].tolist(), dev_df[\"label\"].astype(int).tolist()\n",
    "test_texts, test_labels   = test_df[\"sequence\"].tolist(), test_df[\"label\"].astype(int).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1b60f",
   "metadata": {},
   "source": [
    "# 6) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24eb29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTokenizing train/dev/test...\")\n",
    "enc_train = tokenizer(train_texts, truncation=True, padding=False, max_length=model_max_length)\n",
    "enc_dev = tokenizer(dev_texts, truncation=True, padding=False, max_length=model_max_length)\n",
    "enc_test = tokenizer(test_texts, truncation=True, padding=False, max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd641b4",
   "metadata": {},
   "source": [
    "# 7) Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a073e97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict({\n",
    "    \"input_ids\": enc_train[\"input_ids\"],\n",
    "    \"attention_mask\": enc_train[\"attention_mask\"],\n",
    "    \"labels\": train_labels,\n",
    "})\n",
    "\n",
    "dev_ds = Dataset.from_dict({\n",
    "    \"input_ids\": enc_dev[\"input_ids\"],\n",
    "    \"attention_mask\": enc_dev[\"attention_mask\"],\n",
    "    \"labels\": dev_labels,\n",
    "})\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"input_ids\": enc_test[\"input_ids\"],\n",
    "    \"attention_mask\": enc_test[\"attention_mask\"],\n",
    "    \"labels\": test_labels,\n",
    "})\n",
    "\n",
    "num_labels = len(set(train_labels + dev_labels + test_labels))\n",
    "print(\"num_labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e0edd",
   "metadata": {},
   "source": [
    "# 8) Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528dc9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Num epochs:\", num_train_epochs)\n",
    "print(\"Save strategy:\", save_strategy)\n",
    "print(\"Load best model at end:\", load_best_model_at_end)\n",
    "print(\"Train head only:\", train_head_only)\n",
    "print(\"Use LoRA:\", use_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3fc34",
   "metadata": {},
   "source": [
    "# 9) Training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14646a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if train_head_only and use_lora:\n",
    "    raise ValueError(\"Choose only ONE strategy: head-only or LoRA (or both False for full FT)\")\n",
    "\n",
    "if train_head_only:\n",
    "    print(\"\\n[Head-only] Freezing backbone and leaving only classifier trainable...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    # Reactivate only classifier params\n",
    "    head_substrings = [\"classifier\", \"score\"]\n",
    "    unlocked = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(s in name for s in head_substrings):\n",
    "            param.requires_grad = True\n",
    "            unlocked.append(name)\n",
    "    if not unlocked:\n",
    "        print(\"[WARNING] No classifier layer found (adjust head_substrings)\")\n",
    "    else:\n",
    "        print(\"Reactivated classifier parameters:\")\n",
    "        for n in unlocked:\n",
    "            print(\"  -\", n)\n",
    "elif use_lora:\n",
    "    print(\"\\n[LoRA] Applying efficient adaptation...\")\n",
    "    lconf = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[s.strip() for s in lora_target_modules.split(\",\") if s.strip()],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    model = get_peft_model(model, lconf)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"\\n[Full fine-tuning] All parameters will be updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a7e0b",
   "metadata": {},
   "source": [
    "# 10) Data collator and TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88593f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=fp16,\n",
    "    save_strategy=save_strategy,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps if evaluation_strategy == \"steps\" else None,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,\n",
    "    save_total_limit=save_total_limit,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    dataloader_pin_memory=False,\n",
    "    seed=seed,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec59c9a",
   "metadata": {},
   "source": [
    "# 11) Metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991cfad1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"matthews_correlation\": matthews_corrcoef(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85aa13",
   "metadata": {},
   "source": [
    "# 12) Trainer and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1f3fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1d03c",
   "metadata": {},
   "source": [
    "# 13) Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb7a5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on test set...\")\n",
    "results = trainer.evaluate(eval_dataset=test_ds)\n",
    "print(\"\\nMetrics (test):\\n\", json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f499cc",
   "metadata": {},
   "source": [
    "# 14) Save results and model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9cb376",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    results_path = os.path.join(output_dir, \"results\", run_name)\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    with open(os.path.join(results_path, \"eval_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nResults saved at: {os.path.join(results_path, 'eval_results.json')}\")\n",
    "\n",
    "if save_model:\n",
    "    final_model_dir = os.path.join(output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_dir)\n",
    "    print(f\"Model saved at: {final_model_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
